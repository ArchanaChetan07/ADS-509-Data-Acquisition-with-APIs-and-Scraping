{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfe6ef2",
   "metadata": {},
   "source": [
    " Project Title: Lyrics Scraper Using Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688b740",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "1. Project Objective\n",
    "\n",
    "2. Setup and Imports\n",
    "\n",
    "3. Define Utility Functions\n",
    "\n",
    "4. Scraping Process\n",
    "\n",
    "5. Data Saving and CSV Export\n",
    "\n",
    "6. Results and Data Preview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bace6",
   "metadata": {},
   "source": [
    "Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30295645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c6445",
   "metadata": {},
   "source": [
    "Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d40805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(path, text, replace=False):\n",
    "    \"\"\"Save lyrics text to a file with optional replacement.\"\"\"\n",
    "    file_path = path + \".txt\"\n",
    "    if not replace and os.path.exists(file_path):\n",
    "        file_path = path + \"_2.txt\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "\n",
    "def get_lyrics(song_url, artist_name, save=True, by_decade=False, replace=False, folder=\"songs\"):\n",
    "    \"\"\"Fetch lyrics from a given song URL.\"\"\"\n",
    "    try:\n",
    "        song = urlopen(song_url)\n",
    "        soup = BeautifulSoup(song.read(), \"html.parser\")\n",
    "        all_divs = soup.find_all(\"div\")\n",
    "        lyrics_div = next((div for div in all_divs if not div.get(\"class\") and not div.get(\"id\")), None)\n",
    "        lyrics = lyrics_div.get_text(strip=True, separator=\"\\n\") if lyrics_div else \"Lyrics Not Found\"\n",
    "\n",
    "        title = soup.find_all(\"b\")[1].get_text().replace('\"', '').replace(\" \", \"_\")\n",
    "        album = soup.find_all(class_=\"songinalbum_title\")\n",
    "\n",
    "        year, decade = None, \"others\"\n",
    "        if album:\n",
    "            try:\n",
    "                year = int(album[0].get_text().split('(')[-1].split(')')[0])\n",
    "                decade = f\"{str(year)[:3]}0s\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        if save:\n",
    "            save_file(f\"{folder}/all/{title}\", lyrics, replace)\n",
    "            if by_decade:\n",
    "                save_file(f\"{folder}/decades/{decade}/{title}\", lyrics, replace)\n",
    "\n",
    "        return {\"Artist\": artist_name, \"Song_Title\": title, \"Lyrics\": lyrics, \"Album_Year\": year, \"Decade\": decade}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching lyrics from {song_url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8aafc",
   "metadata": {},
   "source": [
    "## A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).) \n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again. \n",
    "\n",
    "## Part 1: Finding Links to Songs Lyrics\n",
    "\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages. \n",
    "\n",
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know? \n",
    "\n",
    "A: To determine whether scraping is allowed or disallowed, we must check the robots.txt file of the website. This file outlines the rules set by the website administrators for web crawlers and automated bots. If the file explicitly disallows crawling paths related to song lyrics or specific directories (e.g., /lyrics/), then scraping those pages would violate the site's policy. If no such disallow rules exist for the relevant paths, scraping is technically permitted, but ethical considerations and rate limiting should still be followed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb020b74",
   "metadata": {},
   "source": [
    "Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cefd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_artist(az_url, artist_name, song_limit=20, sleep=\"random\", by_decade=True, replace=False, folder=\"songs\"):\n",
    "    \"\"\"Scrape songs for a specific artist up to a defined limit.\"\"\"\n",
    "    base_url = \"https://www.azlyrics.com/\"\n",
    "    artist_data = []\n",
    "    try:\n",
    "        main_page = urlopen(az_url)\n",
    "        soup = BeautifulSoup(main_page.read(), \"html.parser\")\n",
    "        song_divs = soup.find_all('div', {\"class\": \"listalbum-item\"})\n",
    "        urls = [base_url + d.a['href'].split(\"/\", 1)[1] for d in song_divs][:song_limit]\n",
    "\n",
    "        for idx, url in enumerate(urls, 1):\n",
    "            song_data = get_lyrics(url, artist_name, save=True, by_decade=by_decade, replace=replace, folder=folder)\n",
    "            if song_data:\n",
    "                artist_data.append(song_data)\n",
    "            rt = random.randint(5, 15) if sleep == \"random\" else sleep\n",
    "            print(f\"Downloaded: {idx}/{len(urls)} - ETA: {round(rt * (len(urls) - idx) / 60, 2)} min\")\n",
    "            time.sleep(rt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping artist page {az_url}: {e}\")\n",
    "\n",
    "    return artist_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2534a1",
   "metadata": {},
   "source": [
    "Run Scraping for Selected Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f55b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Adele ---\n",
      "Downloaded: 1/20 - ETA: 2.85 min\n",
      "Downloaded: 2/20 - ETA: 4.2 min\n",
      "Downloaded: 3/20 - ETA: 1.98 min\n",
      "Downloaded: 4/20 - ETA: 4.0 min\n",
      "Downloaded: 5/20 - ETA: 2.0 min\n",
      "Downloaded: 6/20 - ETA: 1.63 min\n",
      "Downloaded: 7/20 - ETA: 1.52 min\n",
      "Downloaded: 8/20 - ETA: 1.4 min\n",
      "Downloaded: 9/20 - ETA: 0.92 min\n",
      "Downloaded: 10/20 - ETA: 0.83 min\n",
      "Downloaded: 11/20 - ETA: 0.75 min\n",
      "Downloaded: 12/20 - ETA: 1.07 min\n",
      "Downloaded: 13/20 - ETA: 1.28 min\n",
      "Downloaded: 14/20 - ETA: 0.7 min\n",
      "Downloaded: 15/20 - ETA: 0.83 min\n",
      "Downloaded: 16/20 - ETA: 0.8 min\n",
      "Downloaded: 17/20 - ETA: 0.3 min\n",
      "Downloaded: 18/20 - ETA: 0.43 min\n",
      "Downloaded: 19/20 - ETA: 0.15 min\n",
      "Downloaded: 20/20 - ETA: 0.0 min\n",
      "\n",
      "--- Scraping Eminem ---\n",
      "Downloaded: 1/20 - ETA: 2.22 min\n",
      "Downloaded: 2/20 - ETA: 3.3 min\n",
      "Downloaded: 3/20 - ETA: 3.12 min\n",
      "Downloaded: 4/20 - ETA: 2.13 min\n",
      "Downloaded: 5/20 - ETA: 2.5 min\n",
      "Downloaded: 6/20 - ETA: 2.8 min\n",
      "Downloaded: 7/20 - ETA: 1.95 min\n",
      "Downloaded: 8/20 - ETA: 2.6 min\n",
      "Downloaded: 9/20 - ETA: 1.83 min\n",
      "Downloaded: 10/20 - ETA: 0.83 min\n",
      "Downloaded: 11/20 - ETA: 1.5 min\n",
      "Downloaded: 12/20 - ETA: 2.0 min\n",
      "Downloaded: 13/20 - ETA: 1.28 min\n",
      "Downloaded: 14/20 - ETA: 1.3 min\n",
      "Downloaded: 15/20 - ETA: 0.75 min\n",
      "Downloaded: 16/20 - ETA: 0.8 min\n",
      "Downloaded: 17/20 - ETA: 0.25 min\n",
      "Downloaded: 18/20 - ETA: 0.43 min\n",
      "Downloaded: 19/20 - ETA: 0.18 min\n",
      "Downloaded: 20/20 - ETA: 0.0 min\n"
     ]
    }
   ],
   "source": [
    "artist_urls = [\n",
    "    \"https://www.azlyrics.com/a/adele.html\",\n",
    "    \"https://www.azlyrics.com/e/eminem.html\"\n",
    "]\n",
    "artist_names = [\"adele\", \"eminem\"]\n",
    "\n",
    "base_folder = r\"C:\\Users\\archa\\Desktop\\lyrics\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "all_lyrics_data = []\n",
    "\n",
    "for name, url in zip(artist_names, artist_urls):\n",
    "    print(f\"\\n--- Scraping {name.title()} ---\")\n",
    "    artist_lyrics = scrape_artist(\n",
    "        az_url=url,\n",
    "        artist_name=name,\n",
    "        song_limit=20,\n",
    "        sleep=\"random\",\n",
    "        by_decade=True,\n",
    "        replace=False,\n",
    "        folder=os.path.join(base_folder, name)\n",
    "    )\n",
    "    all_lyrics_data.extend(artist_lyrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbbebc",
   "metadata": {},
   "source": [
    "Data Saving and CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c77bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Lyrics saved to: C:\\Users\\archa\\Desktop\\lyrics\\lyrics_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "if all_lyrics_data:\n",
    "    df = pd.DataFrame(all_lyrics_data)\n",
    "    csv_path = os.path.join(base_folder, \"lyrics_dataset.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\n✅ Lyrics saved to: {csv_path}\")\n",
    "else:\n",
    "    print(\"\\n❌ No data was scraped. Please verify artist URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a25dcee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song_Title</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Album_Year</th>\n",
       "      <th>Decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adele</td>\n",
       "      <td>Daydreamer</td>\n",
       "      <td>Daydreamer\\nSitting on the sea\\nSoaking up the...</td>\n",
       "      <td>2008</td>\n",
       "      <td>2000s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adele</td>\n",
       "      <td>Best_For_Last</td>\n",
       "      <td>Wait, do you see my heart on my sleeve?\\nIt's ...</td>\n",
       "      <td>2008</td>\n",
       "      <td>2000s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adele</td>\n",
       "      <td>Chasing_Pavements</td>\n",
       "      <td>I've made up my mind\\nDon't need to think it o...</td>\n",
       "      <td>2008</td>\n",
       "      <td>2000s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adele</td>\n",
       "      <td>Cold_Shoulder</td>\n",
       "      <td>You say it's all in my head\\nAnd the things I ...</td>\n",
       "      <td>2008</td>\n",
       "      <td>2000s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adele</td>\n",
       "      <td>Crazy_For_You</td>\n",
       "      <td>Found myself today\\nSinging out loud your name...</td>\n",
       "      <td>2008</td>\n",
       "      <td>2000s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Artist         Song_Title  \\\n",
       "0  adele         Daydreamer   \n",
       "1  adele      Best_For_Last   \n",
       "2  adele  Chasing_Pavements   \n",
       "3  adele      Cold_Shoulder   \n",
       "4  adele      Crazy_For_You   \n",
       "\n",
       "                                              Lyrics  Album_Year Decade  \n",
       "0  Daydreamer\\nSitting on the sea\\nSoaking up the...        2008  2000s  \n",
       "1  Wait, do you see my heart on my sleeve?\\nIt's ...        2008  2000s  \n",
       "2  I've made up my mind\\nDon't need to think it o...        2008  2000s  \n",
       "3  You say it's all in my head\\nAnd the things I ...        2008  2000s  \n",
       "4  Found myself today\\nSinging out loud your name...        2008  2000s  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the dataset\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyrics_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
